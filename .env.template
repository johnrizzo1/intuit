# OpenAI API Key - Get from https://platform.openai.com/api-keys
OPENAI_API_KEY=
# OPENAI_API_BASE=https://api.openai.com/v1  # Default OpenAI API base URL

# Google API Credentials - Get from https://console.cloud.google.com/apis/credentials
GOOGLE_CLIENT_ID=
GOOGLE_CLIENT_SECRET=

# Weather API Key - Get from https://openweathermap.org/api
WEATHER_API_KEY=

# For searching the web
SERPER_API_KEY=

# ============================================================================
# Hardware-Accelerated Audio Pipeline Configuration
# ============================================================================

# Speech-to-Text (STT) Configuration
# Provider: whisper (local, CPU/CUDA) or google (cloud API)
# IMPORTANT: OpenAI Whisper does NOT support Apple Silicon MPS due to
# sparse tensor limitations. On macOS, Whisper will automatically use CPU.
# For GPU-accelerated STT on Apple Silicon, consider Google STT or
# migrating to faster-whisper library in the future.
STT_PROVIDER=whisper
# Model size: tiny, base, small, medium, large (larger = better quality, slower)
STT_MODEL_SIZE=base
# Device: cuda (NVIDIA GPU), cpu (all platforms), auto (detect)
# Note: MPS is not supported - will auto-fallback to CPU
# STT_DEVICE=
# Language code (e.g., en, es, fr, de)
STT_LANGUAGE=en
# Use FP16 precision for faster inference (CUDA only, not MPS)
STT_USE_FP16=true
# Compute type: float16, float32, int8
STT_COMPUTE_TYPE=float16

# Text-to-Speech (TTS) Configuration
# Provider: coqui (local, CPU/CUDA), gtts (cloud API, simple)
# IMPORTANT: Coqui TTS does NOT support Apple Silicon MPS due to sparse
# tensor limitations. On macOS, Coqui will automatically use CPU.
# For best performance on Apple Silicon, use gtts provider instead.
TTS_PROVIDER=gtts
# Model identifier (see Coqui TTS documentation for available models)
# TTS_MODEL=tts_models/en/ljspeech/tacotron2-DDC
# Device: cuda (NVIDIA GPU), cpu (all platforms)
# Note: MPS is not supported - will auto-fallback to CPU
TTS_DEVICE=cpu
# Use GPU acceleration (only works with CUDA, not MPS)
TTS_USE_GPU=false
# Optional vocoder model
# TTS_VOCODER=
# Optional speaker ID for multi-speaker models
# TTS_SPEAKER=

# LLM Configuration
# Provider: ollama (local, GPU-accelerated) or openai (cloud API)
LLM_PROVIDER=ollama
# Model identifier
# For Ollama: llama3.2:3b, llama3.1:8b, mistral:7b, phi3:mini
# For OpenAI: gpt-4, gpt-4-turbo, gpt-3.5-turbo
LLM_MODEL=llama3.2:3b
# Ollama server URL (only used when LLM_PROVIDER=ollama)
OLLAMA_BASE_URL=http://localhost:11434
# Sampling temperature (0.0-2.0, higher = more creative)
LLM_TEMPERATURE=0.7
# Maximum tokens to generate
LLM_MAX_TOKENS=2000
# Enable streaming responses
LLM_STREAMING=true

# Embedding Model Configuration
# Provider: ollama (local) or openai (cloud API)
# For Ollama: nomic-embed-text (recommended), mxbai-embed-large, all-minilm
# For OpenAI: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
EMBEDDING_MODEL=ollama:nomic-embed-text

# Hardware Detection
# Automatically detect and use best available hardware (GPU if available)
AUTO_DETECT_HARDWARE=true